{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `train` and `test` sets of TCR for R-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *tcr.xlsx* is the file we created using CREST. To see how you can create this file, check the CREST repository:\n",
    "# https://github.com/phosseini/crest\n",
    "df = pd.read_excel('../data/tcr.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_idx(string):\n",
    "    \"\"\"\n",
    "    converting string of span indices to a dictionary in form of {\"span1\": [], \"span2\": [], \"signal\": []}\n",
    "    :param string: string of span indices in form of:\n",
    "        span1 start_1:end_1 ... start_n:end_n\n",
    "        span2 start_1:end_1 ... start_n:end_n\n",
    "        signal start_1:end_1 ... start_n:end_n\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    idx_val = {\"span1\": [], \"span2\": [], \"signal\": []}\n",
    "    string = string.strip().split('\\n')\n",
    "    for index, (key, value) in enumerate(idx_val.items()):\n",
    "        spans = string[index].split(' ')\n",
    "        for span in spans[1:]:\n",
    "            span = span.split(':')\n",
    "            idx_val[key].append([int(span[0]), int(span[1])])\n",
    "    return idx_val\n",
    "\n",
    "def add_labels(x):\n",
    "    x['idx'] = string_to_idx(x['idx'])\n",
    "    x1 = x['idx']['span1'][0]\n",
    "    x2 = x['idx']['span2'][0]\n",
    "    text = x['context']\n",
    "    tagged_text = text[:x1[0]] + \" <e1> \" + text[x1[0]:x1[1]] + \" </e1> \"\n",
    "    tagged_text += text[x1[1]:x2[0]] + \" <e2> \" + text[x2[0]:x2[1]] + \" </e2> \"\n",
    "    tagged_text += text[x2[1]:]\n",
    "    return tagged_text\n",
    "\n",
    "df['context_tagged'] = df.apply(lambda row: add_labels(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "def find_sent(sents):\n",
    "    for i in range(len(sents)):\n",
    "        if '<e1>' in sents[i].text and '<e2>' in sents[i].text:\n",
    "            return sents[i].text\n",
    "        else:\n",
    "            txt = sents[i-1].text + \" \" + sents[i].text + \" \" + sents[i+1].text\n",
    "            if '<e1>' in txt and '<e2>' in txt:\n",
    "                return txt\n",
    "    return\n",
    "\n",
    "def create_split(data, file_name):\n",
    "    with open(file_name, 'w') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        for idx, row in data.iterrows():\n",
    "            label = 'Cause-Effect(e1,e2)' if row['direction'] == 0 else 'Cause-Effect(e2,e1)'\n",
    "            sent = find_sent(list(nlp(row['context_tagged']).sents))\n",
    "            sent = sent.replace('\\n', ' ')\n",
    "            sent = sent.replace('\"', ' ')\n",
    "            sent = re.sub(' +', ' ', sent)\n",
    "            writer.writerow([label, sent.strip()])\n",
    "\n",
    "train = df.loc[df['split'] == 1]\n",
    "test = df.loc[df['split'] == 2]\n",
    "create_split(train, '../data/train.tsv')\n",
    "create_split(test, '../data/test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking tags and sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- avg. sequence (seq.) length in [train]: 70.28125\n",
      "- min seq. length: 25\n",
      "- max seq. length: 115\n",
      "\n",
      "- avg. sequence (seq.) length in [test]: 65.36363636363636\n",
      "- min seq. length: 20\n",
      "- max seq. length: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = ['train', 'test']\n",
    "for split in splits:\n",
    "    data = pd.read_csv('../data/{}.tsv'.format(split), sep='\\t', names=[\"label\", \"text\"])\n",
    "    sequence_lengths = list()\n",
    "    for idx, row in data.iterrows():\n",
    "        sequence_lengths.append(len(row['text'].split(' ')))\n",
    "        if not any(tag in row['text'] for tag in ['<e1>', '<e2>', '</e1>', '</e2>']):\n",
    "            print('missing tag!')\n",
    "    print('- avg. sequence (seq.) length in [{}]: {}'.format(split, statistics.mean(sequence_lengths)))\n",
    "    print(\"- min seq. length: {}\\n- max seq. length: {}\\n\".format(min(sequence_lengths), max(sequence_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbert",
   "language": "python",
   "name": "rbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
